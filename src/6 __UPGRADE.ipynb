{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import Image, display\n",
    "import tensorflow\n",
    "os.environ['PATH'] += ';C:\\\\Program Files\\\\Graphviz\\\\bin'\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# !pip install talos\n",
    "import talos\n",
    "\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_variables = dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "# Input parameters\n",
    "TRAINING_FILES = [\"../data/original/training_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\difference_preprocessed_training_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\ole_RemoveOutliers_and_preprocessing_pipe_training_dataset.csv\"]\n",
    "EVALUATION_FILES = [\"../data/original/evaluation_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\difference_preprocessed_evaluation_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\ole_RemoveOutliers_and_preprocessing_pipe_evaluation_dataset.csv\"]\n",
    "\n",
    "FILE_N = 0\n",
    "INPUT_TRAINING_FILE = TRAINING_FILES[FILE_N] # \"../data/original/training_dataset.csv\"\n",
    "INPUT_EVALUATION_FILE = EVALUATION_FILES[FILE_N] # \"../data/original/evaluation_dataset.csv\"\n",
    "\n",
    "\n",
    "# Output parameters\n",
    "METHOD_NAME = \"keras_tensorflow_resnet\"\n",
    "TIMESTAMP = time.strftime(\"%d_%m_%Y-%H_%M_%S\")\n",
    "OUTPUT_MODEL_FOLDER = f\"../data/models/keras_models\"\n",
    "OUTPUT_MODEL = f\"../data/models/{METHOD_NAME}_keras_model_{TIMESTAMP}.pkl\"\n",
    "OUTPUT_RESULTS = f\"../data/results/{METHOD_NAME}_model_{TIMESTAMP}.txt\"\n",
    "OUTPUT_MODEL_ARCHITECTURE_RESULTS = f\"../img/{METHOD_NAME}_model_{TIMESTAMP}.png\"\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "# Bankrupt companies must have higher weight, because there are fewer samples\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_SCORING = \"accuracy\"\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 5\n",
    "\n",
    "# Cross validation parameters\n",
    "CROSS_VALIDATION_CV = 5\n",
    "\n",
    "# Cost parameters\n",
    "PUNISHMENT_FOR_FALSE_BANKRUPT_PREDICTION = 100\n",
    "\n",
    "# Other constants\n",
    "LABELS = [\"Operational\", \"Bankrupt\"]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "HYPER_PARAMETER_K_FEATURES = list(range(30, 90, 10))\n",
    "HYPER_PARAMETER_SCORE_FUNC = [chi2, f_classif]\n",
    "HYPER_PARAMETER_HIDDEN_LAYER_SIZES = [(100,)]\n",
    "#HYPER_PARAMETER_HIDDEN_LAYER_SIZES = [(100,), (100, 100), (100, 100, 100),\n",
    "#    (200,), (200, 200), (200, 200, 200), (100, 500)]\n",
    "HYPER_PARAMETER_ALPHA = list(10.0 ** -numpy.arange(1, 7))\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_BETA = 2\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.328621</td>\n",
       "      <td>0.286088</td>\n",
       "      <td>0.343487</td>\n",
       "      <td>0.543183</td>\n",
       "      <td>0.543183</td>\n",
       "      <td>0.997320</td>\n",
       "      <td>0.795251</td>\n",
       "      <td>0.807303</td>\n",
       "      <td>0.303237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631838</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.623315</td>\n",
       "      <td>0.543178</td>\n",
       "      <td>0.823296</td>\n",
       "      <td>0.282771</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.565019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.529128</td>\n",
       "      <td>0.546991</td>\n",
       "      <td>0.572675</td>\n",
       "      <td>0.613651</td>\n",
       "      <td>0.613637</td>\n",
       "      <td>0.999090</td>\n",
       "      <td>0.797475</td>\n",
       "      <td>0.809396</td>\n",
       "      <td>0.303420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804139</td>\n",
       "      <td>0.089456</td>\n",
       "      <td>0.623503</td>\n",
       "      <td>0.613651</td>\n",
       "      <td>0.840650</td>\n",
       "      <td>0.278799</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>0.565739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.502218</td>\n",
       "      <td>0.564054</td>\n",
       "      <td>0.567750</td>\n",
       "      <td>0.627474</td>\n",
       "      <td>0.627474</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.797438</td>\n",
       "      <td>0.809428</td>\n",
       "      <td>0.303533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813615</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.623942</td>\n",
       "      <td>0.627473</td>\n",
       "      <td>0.841078</td>\n",
       "      <td>0.276533</td>\n",
       "      <td>0.026850</td>\n",
       "      <td>0.565424</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.486082</td>\n",
       "      <td>0.545083</td>\n",
       "      <td>0.537877</td>\n",
       "      <td>0.604873</td>\n",
       "      <td>0.604844</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.797412</td>\n",
       "      <td>0.809330</td>\n",
       "      <td>0.303505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801497</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.623702</td>\n",
       "      <td>0.604872</td>\n",
       "      <td>0.840925</td>\n",
       "      <td>0.291015</td>\n",
       "      <td>0.027270</td>\n",
       "      <td>0.566729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.577519</td>\n",
       "      <td>0.563574</td>\n",
       "      <td>0.610113</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>0.797564</td>\n",
       "      <td>0.809432</td>\n",
       "      <td>0.303567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819700</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.623851</td>\n",
       "      <td>0.610110</td>\n",
       "      <td>0.842184</td>\n",
       "      <td>0.281070</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.565285</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "0          0                                           0.328621          \n",
       "1          0                                           0.529128          \n",
       "2          0                                           0.502218          \n",
       "3          0                                           0.486082          \n",
       "4          0                                           0.522400          \n",
       "\n",
       "    ROA(A) before interest and % after tax  \\\n",
       "0                                 0.286088   \n",
       "1                                 0.546991   \n",
       "2                                 0.564054   \n",
       "3                                 0.545083   \n",
       "4                                 0.577519   \n",
       "\n",
       "    ROA(B) before interest and depreciation after tax  \\\n",
       "0                                           0.343487    \n",
       "1                                           0.572675    \n",
       "2                                           0.567750    \n",
       "3                                           0.537877    \n",
       "4                                           0.563574    \n",
       "\n",
       "    Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                 0.543183                      0.543183   \n",
       "1                 0.613651                      0.613637   \n",
       "2                 0.627474                      0.627474   \n",
       "3                 0.604873                      0.604844   \n",
       "4                 0.610113                      0.610192   \n",
       "\n",
       "    Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                0.997320                    0.795251   \n",
       "1                0.999090                    0.797475   \n",
       "2                0.999006                    0.797438   \n",
       "3                0.998997                    0.797412   \n",
       "4                0.999094                    0.797564   \n",
       "\n",
       "    After-tax net Interest Rate   Non-industry income and expenditure/revenue  \\\n",
       "0                      0.807303                                      0.303237   \n",
       "1                      0.809396                                      0.303420   \n",
       "2                      0.809428                                      0.303533   \n",
       "3                      0.809330                                      0.303505   \n",
       "4                      0.809432                                      0.303567   \n",
       "\n",
       "   ...   Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0  ...                     0.631838                    0.000809   \n",
       "1  ...                     0.804139                    0.089456   \n",
       "2  ...                     0.813615                    0.000215   \n",
       "3  ...                     0.801497                    0.002832   \n",
       "4  ...                     0.819700                    0.007407   \n",
       "\n",
       "    No-credit Interval   Gross Profit to Sales  \\\n",
       "0             0.623315                0.543178   \n",
       "1             0.623503                0.613651   \n",
       "2             0.623942                0.627473   \n",
       "3             0.623702                0.604872   \n",
       "4             0.623851                0.610110   \n",
       "\n",
       "    Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                             0.823296              0.282771   \n",
       "1                             0.840650              0.278799   \n",
       "2                             0.841078              0.276533   \n",
       "3                             0.840925              0.291015   \n",
       "4                             0.842184              0.281070   \n",
       "\n",
       "    Degree of Financial Leverage (DFL)  \\\n",
       "0                             0.026763   \n",
       "1                             0.026929   \n",
       "2                             0.026850   \n",
       "3                             0.027270   \n",
       "4                             0.026818   \n",
       "\n",
       "    Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "0                                           0.565019                   1   \n",
       "1                                           0.565739                   1   \n",
       "2                                           0.565424                   1   \n",
       "3                                           0.566729                   1   \n",
       "4                                           0.565285                   1   \n",
       "\n",
       "    Equity to Liability  \n",
       "0              0.022198  \n",
       "1              0.033738  \n",
       "2              0.062519  \n",
       "3              0.016158  \n",
       "4              0.025383  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_dataset = pd.read_csv(INPUT_TRAINING_FILE, engine=\"python\", delimiter=\",\")\n",
    "training_features = training_dataset.loc[:, training_dataset.columns != \"Bankrupt?\"]\n",
    "training_targets = training_dataset[\"Bankrupt?\"]\n",
    "\n",
    "evaluation_dataset = pd.read_csv(INPUT_EVALUATION_FILE, engine=\"python\", delimiter=\",\")\n",
    "evaluation_features = evaluation_dataset.loc[:, evaluation_dataset.columns != \"Bankrupt?\"]\n",
    "evaluation_targets = evaluation_dataset[\"Bankrupt?\"]\n",
    "\n",
    "training_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.functional.Functional'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def keras_model_for_sklearn(optimizer='adam',#'adagrad',\n",
    "                    loss = 'binary_crossentropy', \n",
    "                    kernel_initializer='glorot_uniform', \n",
    "                    dropout=0.2, layers_n  = 95,\n",
    "                    n_activator = 'relu',\n",
    "                    neurons = 213 ,\n",
    "                    n_out = 1,\n",
    "                    f_activator = 'relu',\n",
    "                    batch_norm = True,\n",
    "                    metrics = ['accuracy', 'MeanSquaredError', 'AUC', \"RootMeanSquaredError\"]\n",
    "                  ):\n",
    "        \n",
    "    \n",
    "    input =tensorflow.keras.layers.Input(shape=(layers_n,))\n",
    "    output1 =tensorflow.keras.layers.Dense(layers_n, input_shape=(layers_n,), activation=n_activator)(input)\n",
    "    if batch_norm:\n",
    "        output1 =tensorflow.keras.layers.BatchNormalization(axis=1, name='dense_relu' + '2a')(output1)\n",
    "    output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "    dense_name_base = \"dense\"\n",
    "    bn_name_base = \"dense_additionall\"\n",
    "    shortcut1 =tensorflow.keras.layers.Dense(layers_n, activation=n_activator, kernel_initializer='he_normal',\n",
    "                                name=dense_name_base + '1')(output1)\n",
    "    output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "\n",
    "    # if batch_norm:\n",
    "    #     output1 =tensorflow.keras.layers.BatchNormalization(axis=1, name=bn_name_base + '2a')(output1)\n",
    "    # shortcut2 =tensorflow.keras.layers.Dense(neurons, activation=n_activator, kernel_initializer='he_normal',\n",
    "    #                             name=dense_name_base + '2')(output1)\n",
    "    \n",
    "    # output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "    # shortcut1 =tensorflow.keras.layers.Dense(neurons, activation=n_activator, kernel_initializer='he_normal',\n",
    "    #                             name=dense_name_base + '22')(output1)\n",
    "    \n",
    "    output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "    output =tensorflow.keras.layers.Dense(n_out, activation=f_activator, name='fc1000')(output1)\n",
    "    \n",
    "    # Final model\n",
    "    model =tensorflow.keras.models.Model(inputs=input, outputs=output)\n",
    "\n",
    "    # Plot model information\n",
    "    # plot_= True\n",
    "    # # plot_model(model, to_file=OUTPUT_MODEL_ARCHITECTURE_RESULTS, show_shapes=True, show_layer_names=True)\n",
    "    # if plot_:\n",
    "    #     plot_model(model, to_file=OUTPUT_MODEL_ARCHITECTURE_RESULTS, show_shapes=True, show_layer_names=True)\n",
    "    #     im = cv2.imread(OUTPUT_MODEL_ARCHITECTURE_RESULTS, 0)\n",
    "    #     model.summary()\n",
    "    #     display(Image(filename=OUTPUT_MODEL_ARCHITECTURE_RESULTS))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "print(type(keras_model_for_sklearn()))# <class 'keras.engine.functional.Functional'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dense, Dropout,     BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score, accuracy_score, precision_score, precision_recall_curve, average_precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.sequential.Sequential'>\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "[[ 277 1835]\n",
      " [   0    0]]\n",
      "[CV 1/5] END clf__dropout=0.2, clf__epochs=10, clf__kernel_initializer=uniform, clf__layers_n=64, clf__metrics=accuracy, clf__optimizer=adam, selection__k=30, selection__score_func=<function chi2 at 0x000001BDE4872EE0>;, score=0.000 total time=   6.9s\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "[[1167  945]\n",
      " [   0    0]]\n",
      "[CV 2/5] END clf__dropout=0.2, clf__epochs=10, clf__kernel_initializer=uniform, clf__layers_n=64, clf__metrics=accuracy, clf__optimizer=adam, selection__k=30, selection__score_func=<function chi2 at 0x000001BDE4872EE0>;, score=0.000 total time=   7.7s\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "[[ 292  763]\n",
      " [   0 1057]]\n",
      "[CV 3/5] END clf__dropout=0.2, clf__epochs=10, clf__kernel_initializer=uniform, clf__layers_n=64, clf__metrics=accuracy, clf__optimizer=adam, selection__k=30, selection__score_func=<function chi2 at 0x000001BDE4872EE0>;, score=0.874 total time=   6.5s\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "[[   0    0]\n",
      " [   1 2110]]\n",
      "[CV 4/5] END clf__dropout=0.2, clf__epochs=10, clf__kernel_initializer=uniform, clf__layers_n=64, clf__metrics=accuracy, clf__optimizer=adam, selection__k=30, selection__score_func=<function chi2 at 0x000001BDE4872EE0>;, score=1.000 total time=   6.8s\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "[[2111]]\n",
      "[CV 5/5] END clf__dropout=0.2, clf__epochs=10, clf__kernel_initializer=uniform, clf__layers_n=64, clf__metrics=accuracy, clf__optimizer=adam, selection__k=30, selection__score_func=<function chi2 at 0x000001BDE4872EE0>;, score=1.000 total time=   6.8s\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocess', StandardScaler()),\n",
       "                                       ('min_max_scaler', MinMaxScaler()),\n",
       "                                       ('selection', SelectKBest()),\n",
       "                                       ('clf',\n",
       "                                        <keras.wrappers.scikit_learn.KerasRegressor object at 0x000001BDE6D2F8E0>)]),\n",
       "             param_grid={'clf__dropout': [0.2], 'clf__epochs': [10],\n",
       "                         'clf__kernel_initializer': ['uniform'],\n",
       "                         'clf__layers_n': [64], 'clf__metrics': ['accuracy'],\n",
       "                         'clf__optimizer': ['adam'], 'selection__k': [30],\n",
       "                         'selection__score_func': [<function chi2 at 0x000001BDE4872EE0>]},\n",
       "             scoring=make_scorer(custom_scorer), verbose=3)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Add\n",
    "\n",
    "def _create_shortcut(inputs):\n",
    "    # here create model as in case you know inputs, e.g.:\n",
    "    aux = Dense(10, activation='relu')(inputs)\n",
    "    output = Dense(10, activation='relu')(aux)\n",
    "    return output  \n",
    "    \n",
    "    \n",
    "# create a function that returns a model, taking as parameters things you\n",
    "# want to verify using cross-valdiation and model selection\n",
    "def create_model(optimizer='adam', kernel_initializer='normal', dropout=0.2, layers_n = 64, metrics=\"accuracy\", batchn = True, hidden_layer_sizes = [20,90]):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(layers_n,activation='relu',kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(dropout))\n",
    "    if batchn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(1,activation='relu',kernel_initializer=kernel_initializer))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=[metrics])\n",
    "    return model\n",
    "\n",
    "def create_model1(optimizer='adam', kernel_initializer='normal', dropout=0.2, layers_n = 64, metrics=\"accuracy\", batchn = True, hidden_layer_sizes = [20,90]):\n",
    "    # model = Sequential()\n",
    "    \n",
    "    # model.add(Dense(layers_n,activation='relu',kernel_initializer=kernel_initializer))\n",
    "    # model.add(Dropout(dropout))\n",
    "    # model.add(_create_shortcut(model))\n",
    "    # if batchn:\n",
    "    #     model.add(BatchNormalization())\n",
    "    # model.add(Dense(1,activation='relu',kernel_initializer=kernel_initializer))\n",
    "    # model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=[metrics])\n",
    "\n",
    "\n",
    "    input1 = Input(shape=(95,))\n",
    "    x1 = Dense(8, activation='relu')(input1)\n",
    "    input2 = Input(shape=(32,))\n",
    "    x2 = Dense(8, activation='relu')(input2)\n",
    "\n",
    "    added = Add()([x1, x2])\n",
    "\n",
    "    out = Dense(4, activation='relu')(added)\n",
    "    model = Model(inputs=[input1, input2], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=[metrics])\n",
    "    return model\n",
    "print(type(create_model())) # <class 'keras.engine.sequential.Sequential'>\n",
    "\n",
    "\n",
    "HYPER_PARAMETER_K_FEATURES = list([20,25,30,35,40,95])\n",
    "HYPER_PARAMETER_SCORE_FUNC = [chi2, f_classif]\n",
    "HYPER_PARAMETER_CLASIFIER_OPTIMIZERS = ['rmsprop','adam','adagrad']\n",
    "HYPER_PARAMETER_EPOCHS = [10,20,40,60]\n",
    "HYPER_PARAMETER_DROPUT = [0,0.1,0.2,0.3]\n",
    "HYPER_PARAMETER_LAYERS_N = list(range(20,90,10))\n",
    "HYPER_PARAMETER_BATCH_NUMBER = list(range(20,160,30))\n",
    "HYPER_PARAMETER_KERNEL_INITIALIZER = ['glorot_uniform','normal','uniform']\n",
    "HYPER_PARAMETER_CLASIFIER_METRICS = ['accuracy']\n",
    "\n",
    "param_grid = {\n",
    "    \"selection__k\": HYPER_PARAMETER_K_FEATURES,\n",
    "    \"selection__score_func\": HYPER_PARAMETER_SCORE_FUNC,\n",
    "    'clf__optimizer':HYPER_PARAMETER_CLASIFIER_OPTIMIZERS,\n",
    "    'clf__epochs':HYPER_PARAMETER_EPOCHS,\n",
    "    'clf__dropout':HYPER_PARAMETER_DROPUT,\n",
    "    'clf__layers_n' : HYPER_PARAMETER_LAYERS_N,\n",
    "    'clf__batchn' : HYPER_PARAMETER_BATCH_NUMBER,\n",
    "    'clf__kernel_initializer':HYPER_PARAMETER_KERNEL_INITIALIZER,\n",
    "    'clf__metrics':HYPER_PARAMETER_CLASIFIER_METRICS,\n",
    "}\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'clf__optimizer':['adam'],\n",
    "    'clf__epochs':[10],\n",
    "    'clf__dropout':[0.2],\n",
    "    'clf__layers_n' : [64],\n",
    "    'clf__kernel_initializer':['uniform'],\n",
    "    'clf__metrics':['accuracy'],\n",
    "    \"selection__k\": [30],\n",
    "    \"selection__score_func\": [chi2],\n",
    "}\n",
    "\n",
    "\n",
    "# Using keras regresor for our custom model\n",
    "clf = KerasRegressor(build_fn=create_model,verbose=3)\n",
    "\n",
    "# Definng standart scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Do feature selection with SelectKBest.\n",
    "feature_selection = SelectKBest(f_classif,  k=64)  \n",
    "\n",
    "# Oversample data with SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"minority\", random_state=RANDOM_SEED)\n",
    "X_train, y_train = training_features,training_targets\n",
    "X_train, y_train = smote.fit_resample(training_features,training_targets)\n",
    "# Min max scaler definition \n",
    "min_max_scaler =  MinMaxScaler()\n",
    "\n",
    "# Feature selection \n",
    "feature_selection = SelectKBest()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess',scaler),\n",
    "    (\"min_max_scaler\", min_max_scaler),\n",
    "    (\"selection\", feature_selection),\n",
    "    ('clf',clf)\n",
    "])\n",
    "\n",
    "# Define custom fbeta scorer function that put emphasis on recall\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    confidence = 0\n",
    "    y_pred = (np.where(y_pred > confidence, 1, 0))    \n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    return fbeta_score(y_true, y_pred, beta=HYPER_PARAMETER_OPTIMIZATION_BETA)\n",
    "\n",
    "grid = GridSearchCV(pipeline, \n",
    "                            cv=5,\n",
    "                            param_grid=param_grid\n",
    "                            , verbose = 3 \n",
    "                            , scoring=make_scorer(custom_scorer)\n",
    "                            )\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Acc. Score = 0.574693, with parameters {'clf__dropout': 0.2, 'clf__epochs': 10, 'clf__kernel_initializer': 'uniform', 'clf__layers_n': 64, 'clf__metrics': 'accuracy', 'clf__optimizer': 'adam', 'selection__k': 30, 'selection__score_func': <function chi2 at 0x000001BDE4872EE0>}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model: Acc. Score = %f, with parameters %s\" % (grid.best_score_, grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence 0.0\n",
      "Balanced training. acc: 0.6603528776111178\n",
      "Balanced eval. acc: 0.6367424242424242\n",
      "Confidence 0.02\n",
      "Balanced training. acc: 0.669350793883139\n",
      "Balanced eval. acc: 0.6465909090909091\n",
      "Confidence 0.04\n",
      "Balanced training. acc: 0.6803377232258175\n",
      "Balanced eval. acc: 0.6549242424242424\n",
      "Confidence 0.06\n",
      "Balanced training. acc: 0.6909457929359899\n",
      "Balanced eval. acc: 0.6640151515151516\n",
      "Confidence 0.08\n",
      "Balanced training. acc: 0.7012697179217827\n",
      "Balanced eval. acc: 0.6727272727272727\n",
      "Confidence 0.1\n",
      "Balanced training. acc: 0.7116883578157019\n",
      "Balanced eval. acc: 0.6814393939393939\n",
      "Confidence 0.12\n",
      "Balanced training. acc: 0.7246643002290378\n",
      "Balanced eval. acc: 0.6946969696969697\n",
      "Confidence 0.14\n",
      "Balanced training. acc: 0.7383032469992594\n",
      "Balanced eval. acc: 0.7068181818181818\n",
      "Confidence 0.16\n",
      "Balanced training. acc: 0.7491954614338114\n",
      "Balanced eval. acc: 0.7166666666666667\n",
      "Confidence 0.18\n",
      "Balanced training. acc: 0.7602771056846165\n",
      "Balanced eval. acc: 0.7299242424242425\n",
      "Confidence 0.2\n",
      "Balanced training. acc: 0.7695597048339045\n",
      "Balanced eval. acc: 0.7428030303030303\n",
      "Confidence 0.22\n",
      "Balanced training. acc: 0.7831039366959995\n",
      "Balanced eval. acc: 0.7571969696969697\n",
      "Confidence 0.24\n",
      "Balanced training. acc: 0.7957957343849558\n",
      "Balanced eval. acc: 0.7587121212121212\n",
      "Confidence 0.26\n",
      "Balanced training. acc: 0.8050783335342437\n",
      "Balanced eval. acc: 0.7693181818181818\n",
      "Confidence 0.28\n",
      "Balanced training. acc: 0.8182437057638327\n",
      "Balanced eval. acc: 0.781060606060606\n",
      "Confidence 0.3\n",
      "Balanced training. acc: 0.8287570605658785\n",
      "Balanced eval. acc: 0.7689393939393939\n",
      "Confidence 0.32\n",
      "Balanced training. acc: 0.839743989908557\n",
      "Balanced eval. acc: 0.781439393939394\n",
      "Confidence 0.34\n",
      "Balanced training. acc: 0.8494996254455905\n",
      "Balanced eval. acc: 0.7825757575757576\n",
      "Confidence 0.36\n",
      "Balanced training. acc: 0.8548989133616904\n",
      "Balanced eval. acc: 0.7935606060606061\n",
      "Confidence 0.38\n",
      "Balanced training. acc: 0.8578361518193873\n",
      "Balanced eval. acc: 0.8007575757575758\n",
      "Confidence 0.4\n",
      "Balanced training. acc: 0.8638037291842463\n",
      "Balanced eval. acc: 0.8071969696969696\n",
      "Confidence 0.42\n",
      "Balanced training. acc: 0.8730857901806471\n",
      "Balanced eval. acc: 0.8151515151515152\n",
      "Confidence 0.44\n",
      "Balanced training. acc: 0.8761177435464707\n",
      "Balanced eval. acc: 0.8231060606060606\n",
      "Confidence 0.46\n",
      "Balanced training. acc: 0.8769712540253836\n",
      "Balanced eval. acc: 0.8284090909090909\n",
      "Confidence 0.48\n",
      "Balanced training. acc: 0.8757415746783999\n",
      "Balanced eval. acc: 0.8371212121212122\n",
      "Confidence 0.5\n",
      "Balanced training. acc: 0.8783941302588301\n",
      "Balanced eval. acc: 0.8306818181818182\n",
      "Confidence 0.52\n",
      "Balanced training. acc: 0.8815202603798928\n",
      "Balanced eval. acc: 0.8261363636363637\n",
      "Confidence 0.54\n",
      "Balanced training. acc: 0.8704418450464103\n",
      "Balanced eval. acc: 0.8318181818181818\n",
      "Confidence 0.56\n",
      "Balanced training. acc: 0.8686438762506673\n",
      "Balanced eval. acc: 0.8382575757575758\n",
      "Confidence 0.58\n",
      "Balanced training. acc: 0.8624895598339906\n",
      "Balanced eval. acc: 0.8227272727272728\n",
      "Confidence 0.6\n",
      "Balanced training. acc: 0.8633425321600166\n",
      "Balanced eval. acc: 0.8261363636363637\n",
      "Confidence 0.62\n",
      "Balanced training. acc: 0.8623017444763987\n",
      "Balanced eval. acc: 0.8231060606060606\n",
      "Confidence 0.64\n",
      "Balanced training. acc: 0.8601243778952625\n",
      "Balanced eval. acc: 0.8272727272727273\n",
      "Confidence 0.66\n",
      "Balanced training. acc: 0.8486671029292738\n",
      "Balanced eval. acc: 0.831060606060606\n",
      "Confidence 0.68\n",
      "Balanced training. acc: 0.837588149442904\n",
      "Balanced eval. acc: 0.7890151515151516\n",
      "Confidence 0.7000000000000001\n",
      "Balanced training. acc: 0.8235735719574988\n",
      "Balanced eval. acc: 0.7806818181818183\n",
      "Confidence 0.72\n",
      "Balanced training. acc: 0.8097484242883466\n",
      "Balanced eval. acc: 0.7632575757575757\n",
      "Confidence 0.74\n",
      "Balanced training. acc: 0.8101278220737398\n",
      "Balanced eval. acc: 0.7446969696969696\n",
      "Confidence 0.76\n",
      "Balanced training. acc: 0.7987647238629906\n",
      "Balanced eval. acc: 0.746969696969697\n",
      "Confidence 0.78\n",
      "Balanced training. acc: 0.7863597616628494\n",
      "Balanced eval. acc: 0.7488636363636363\n",
      "Confidence 0.8\n",
      "Balanced training. acc: 0.7780270023592623\n",
      "Balanced eval. acc: 0.7401515151515151\n",
      "Confidence 0.8200000000000001\n",
      "Balanced training. acc: 0.763159990700718\n",
      "Balanced eval. acc: 0.731439393939394\n",
      "Confidence 0.84\n",
      "Balanced training. acc: 0.7505655986843238\n",
      "Balanced eval. acc: 0.7227272727272727\n",
      "Confidence 0.86\n",
      "Balanced training. acc: 0.7289749048545696\n",
      "Balanced eval. acc: 0.7250000000000001\n",
      "Confidence 0.88\n",
      "Balanced training. acc: 0.7224411906524996\n",
      "Balanced eval. acc: 0.7151515151515151\n",
      "Confidence 0.9\n",
      "Balanced training. acc: 0.7043544102705402\n",
      "Balanced eval. acc: 0.7056818181818182\n",
      "Confidence 0.92\n",
      "Balanced training. acc: 0.6887296793469837\n",
      "Balanced eval. acc: 0.6954545454545454\n",
      "Confidence 0.9400000000000001\n",
      "Balanced training. acc: 0.6846580146033168\n",
      "Balanced eval. acc: 0.696969696969697\n",
      "Confidence 0.96\n",
      "Balanced training. acc: 0.6713059033219102\n",
      "Balanced eval. acc: 0.6977272727272728\n",
      "Confidence 0.98\n",
      "Balanced training. acc: 0.660510556407033\n",
      "Balanced eval. acc: 0.6768939393939394\n",
      "max_acc 0.8382575757575758, best_conf0.56\n"
     ]
    }
   ],
   "source": [
    "def output_parameters(confidence, grid, training_features, training_targets,evaluation_features, evaluation_targets, max_acc,best_conf, log = False ):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, roc_curve, recall_score, make_scorer\n",
    "\n",
    "    # Use all training data to calculate confusion matrix for training data\n",
    "    training_estimates_ = grid.predict(training_features)\n",
    "    training_estimates = np.where(training_estimates_ > confidence, 1, 0)\n",
    "\n",
    "    training_accuracy = balanced_accuracy_score(training_targets, training_estimates)\n",
    "    training_confusion_matrix = confusion_matrix(training_targets, training_estimates)\n",
    "\n",
    "    # Use model to estimate manually labeled evaluation Tweets\n",
    "    evaluation_estimates_ = grid.predict(evaluation_features)\n",
    "\n",
    "    evaluation_estimates = np.where(evaluation_estimates_ > confidence, 1, 0)\n",
    "\n",
    "    evaluation_accuracy = balanced_accuracy_score(evaluation_targets, evaluation_estimates)\n",
    "    evaluation_confusion_matrix = confusion_matrix(evaluation_targets, evaluation_estimates)\n",
    "    \n",
    "    # evaluation_confusion_matrix = confusion_matrix_to_string(evaluation_confusion_matrix)\n",
    "    if max_acc <= evaluation_accuracy:\n",
    "        max_acc = evaluation_accuracy\n",
    "        best_conf = confidence\n",
    "\n",
    "    print(f\"Confidence {confidence}\")\n",
    "    if log:\n",
    "        print(training_estimates_)\n",
    "        print(\"\\n\\n\\n\\n\\n\\n_________________________________________________\")\n",
    "        print(training_estimates)\n",
    "        print(training_confusion_matrix)\n",
    "        print(evaluation_estimates_[:5])\n",
    "        print(evaluation_estimates[:5])\n",
    "        print(evaluation_confusion_matrix)\n",
    "\n",
    "    print(f\"Balanced training. acc: {training_accuracy}\")\n",
    "\n",
    "    print(f\"Balanced eval. acc: {evaluation_accuracy}\")\n",
    "\n",
    "    # summarize results\n",
    "\n",
    "    if log:\n",
    "        print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "        means = grid.cv_results_['mean_test_score']\n",
    "        stds = grid.cv_results_['std_test_score']\n",
    "        params = grid.cv_results_['params']\n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "        print(\"Training evaluation\")\n",
    "\n",
    "        y_pred_train_ = grid.predict(training_features)\n",
    "        print(y_pred_train_)\n",
    "        y_pred_train = np.where(y_pred_train_ > confidence, 1, 0)\n",
    "        print(classification_report(training_targets, y_pred_train))\n",
    "        train_conf_matrix = confusion_matrix(training_targets, y_pred_train)\n",
    "\n",
    "        print(\"Validation/evaluation\")\n",
    "        y_pred_test_ = grid.predict(evaluation_features) # prediction on our test set\n",
    "\n",
    "        print(y_pred_test_)\n",
    "        y_pred_test = np.where(y_pred_test_ > confidence, 1, 0)\n",
    "\n",
    "        print(classification_report(evaluation_targets, y_pred_test))\n",
    "        test_conf_matrix = confusion_matrix(evaluation_targets, y_pred_test)\n",
    "        print(test_conf_matrix)\n",
    "    return max_acc, best_conf\n",
    "\n",
    "confidences =  numpy.arange(0, 1,0.02)\n",
    "max_acc = 0\n",
    "best_conf = 0\n",
    "log = False\n",
    "\n",
    "for confidence in confidences:\n",
    "    max_acc,best_conf = output_parameters(confidence, grid, training_features, training_targets,evaluation_features, evaluation_targets, max_acc,best_conf, log = False )\n",
    "\n",
    "print(f\"max_acc {max_acc}, best_conf{best_conf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence 0.56\n",
      "[0.8557158  0.29129314 0.13728543 ... 0.94926924 0.8851565  0.5586289 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_________________________________________________\n",
      "[1 0 0 ... 1 1 0]\n",
      "[[4642  637]\n",
      " [  25  151]]\n",
      "[0.         0.7025697  0.30066675 0.         0.11386048]\n",
      "[0 1 0 0 0]\n",
      "[[1133  187]\n",
      " [   8   36]]\n",
      "Balanced training. acc: 0.8686438762506673\n",
      "Balanced eval. acc: 0.8382575757575758\n",
      "Best: 0.574693 using {'clf__dropout': 0.2, 'clf__epochs': 10, 'clf__kernel_initializer': 'uniform', 'clf__layers_n': 64, 'clf__metrics': 'accuracy', 'clf__optimizer': 'adam', 'selection__k': 30, 'selection__score_func': <function chi2 at 0x000001BDE4872EE0>}\n",
      "0.574693 (0.471484) with: {'clf__dropout': 0.2, 'clf__epochs': 10, 'clf__kernel_initializer': 'uniform', 'clf__layers_n': 64, 'clf__metrics': 'accuracy', 'clf__optimizer': 'adam', 'selection__k': 30, 'selection__score_func': <function chi2 at 0x000001BDE4872EE0>}\n",
      "Training evaluation\n",
      "[0.8557158  0.29129314 0.13728543 ... 0.94926924 0.8851565  0.5586289 ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      5279\n",
      "           1       0.19      0.86      0.31       176\n",
      "\n",
      "    accuracy                           0.88      5455\n",
      "   macro avg       0.59      0.87      0.62      5455\n",
      "weighted avg       0.97      0.88      0.91      5455\n",
      "\n",
      "Validation/evaluation\n",
      "[0.         0.7025697  0.30066675 ... 0.69303936 0.         0.6669114 ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92      1320\n",
      "           1       0.16      0.82      0.27        44\n",
      "\n",
      "    accuracy                           0.86      1364\n",
      "   macro avg       0.58      0.84      0.60      1364\n",
      "weighted avg       0.97      0.86      0.90      1364\n",
      "\n",
      "[[1133  187]\n",
      " [   8   36]]\n"
     ]
    }
   ],
   "source": [
    "confidence = best_conf\n",
    "max_acc,best_conf = output_parameters(best_conf, grid, training_features, training_targets,evaluation_features, evaluation_targets, max_acc,best_conf, log = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS_VALIDATION_CV: 5\n",
      "EVALUATION_FILES: ['../data/original/evaluation_dataset.csv', '..\\\\data\\\\preprocessed\\\\difference_preprocessed_evaluation_dataset.csv', '..\\\\data\\\\preprocessed\\\\ole_RemoveOutliers_and_preprocessing_pipe_evaluation_dataset.csv']\n",
      "FILE_N: 0\n",
      "HYPER_PARAMETER_ALPHA: [0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06]\n",
      "HYPER_PARAMETER_BATCH_NUMBER: [20, 50, 80, 110, 140]\n",
      "HYPER_PARAMETER_CLASIFIER_METRICS: ['accuracy']\n",
      "HYPER_PARAMETER_CLASIFIER_OPTIMIZERS: ['rmsprop', 'adam', 'adagrad']\n",
      "HYPER_PARAMETER_DROPUT: [0, 0.1, 0.2, 0.3]\n",
      "HYPER_PARAMETER_EPOCHS: [10, 20, 40, 60]\n",
      "HYPER_PARAMETER_HIDDEN_LAYER_SIZES: [(100,)]\n",
      "HYPER_PARAMETER_KERNEL_INITIALIZER: ['glorot_uniform', 'normal', 'uniform']\n",
      "HYPER_PARAMETER_K_FEATURES: [20, 25, 30, 35, 40, 95]\n",
      "HYPER_PARAMETER_LAYERS_N: [20, 30, 40, 50, 60, 70, 80]\n",
      "HYPER_PARAMETER_OPTIMIZATION_BETA: 2\n",
      "HYPER_PARAMETER_OPTIMIZATION_CV: 5\n",
      "HYPER_PARAMETER_OPTIMIZATION_SCORING: accuracy\n",
      "HYPER_PARAMETER_SCORE_FUNC: [<function chi2 at 0x000001BDE4872EE0>, <function f_classif at 0x000001BDE4872DC0>]\n",
      "INPUT_EVALUATION_FILE: ../data/original/evaluation_dataset.csv\n",
      "INPUT_TRAINING_FILE: ../data/original/training_dataset.csv\n",
      "LABELS: ['Operational', 'Bankrupt']\n",
      "METHOD_NAME: keras_tensorflow_resnet\n",
      "OUTPUT_MODEL: ../data/models/keras_tensorflow_resnet_keras_model_18_05_2022-19_57_13.pkl\n",
      "OUTPUT_MODEL_ARCHITECTURE_RESULTS: ../img/keras_tensorflow_resnet_model_18_05_2022-19_57_13.png\n",
      "OUTPUT_MODEL_FOLDER: ../data/models/keras_models\n",
      "OUTPUT_RESULTS: ../data/results/keras_tensorflow_resnet_model_18_05_2022-19_57_13.txt\n",
      "PUNISHMENT_FOR_FALSE_BANKRUPT_PREDICTION: 100\n",
      "RANDOM_SEED: 42\n",
      "TIMESTAMP: 18_05_2022-19_57_13\n",
      "TRAINING_FILES: ['../data/original/training_dataset.csv', '..\\\\data\\\\preprocessed\\\\difference_preprocessed_training_dataset.csv', '..\\\\data\\\\preprocessed\\\\ole_RemoveOutliers_and_preprocessing_pipe_training_dataset.csv']\n",
      "best_conf: 0.56\n",
      "confidence: 0.56\n",
      "max_acc: 0.8382575757575758\n",
      "param_grid: {'clf__optimizer': ['adam'], 'clf__epochs': [10], 'clf__dropout': [0.2], 'clf__layers_n': [64], 'clf__kernel_initializer': ['uniform'], 'clf__metrics': ['accuracy'], 'selection__k': [30], 'selection__score_func': [<function chi2 at 0x000001BDE4872EE0>]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_variables = dir()\n",
    "\n",
    "results_string = \"\"\n",
    "\n",
    "for variable in current_variables:\n",
    "    # Skip environment variables and their container variable\n",
    "    # Ignore also underscore variables\n",
    "    if variable in environment_variables or variable == \"environment_variables\" or variable.startswith(\"_\"):\n",
    "        continue\n",
    "\n",
    "    # Get variables value\n",
    "    variable_value = globals()[variable]\n",
    "\n",
    "    # If variable is numerical or string, append it to results\n",
    "    if type(variable_value) is str or type(variable_value) is int or \\\n",
    "        type(variable_value) is float or type(variable_value) is list or \\\n",
    "        type(variable_value) is numpy.float64 or type(variable_value) is dict:\n",
    "        results_string += f\"{variable}: {variable_value}\\n\"\n",
    "\n",
    "# Print results to screen\n",
    "print(results_string)\n",
    "\n",
    "# Save results to file\n",
    "with open(OUTPUT_RESULTS, \"w\") as file:\n",
    "    file.write(results_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEIVID~1\\AppData\\Local\\Temp/ipykernel_15984/3292626468.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUTPUT_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip install import weakref\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open(OUTPUT_MODEL, \"wb\") as handle:\n",
    "    joblib.dump(grid, OUTPUT_MODEL)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d2cc45be89154c81e316490ce08739f90b548aca5356e8d287f3c1f6b5a8c71"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
